# ğŸ–¼ï¸ Image Captioning with BLIP

[![Streamlit](https://img.shields.io/badge/Made%20with-Streamlit-ff4b4b?logo=streamlit&logoColor=white)](https://streamlit.io/)
[![Hugging Face](https://img.shields.io/badge/Model-BLIP-blue?logo=huggingface&logoColor=white)](https://huggingface.co/Salesforce/blip-image-captioning-base)
[![Python](https://img.shields.io/badge/Python-3.9+-3776AB?logo=python&logoColor=white)](https://www.python.org/)
[![License](https://img.shields.io/badge/License-MIT-green)](LICENSE)

> A complete project that demonstrates **automatic image captioning** using the [BLIP model](https://huggingface.co/Salesforce/blip-image-captioning-base).  
> Includes:
> - ğŸ“± **Streamlit Web App** for easy image upload and caption generation
> - ğŸ““ **Step-by-Step Jupyter Notebook** for experimentation
> - ğŸ“‚ **Image Dataset** for testing and extending the model

---

## ğŸ“Œ Project Overview

**Image Captioning** is the task of **generating descriptive text for images**.  
This project uses **BLIP** (Bootstrapping Language-Image Pre-training), a vision-language model capable of understanding visual content and generating natural language descriptions.

We provide:
- A **Streamlit app** for interactive caption generation
- A **Jupyter Notebook** for in-depth understanding
- A **sample dataset** for quick testing

---

## ğŸš€ Features

âœ… Upload an image and get an instant caption  
âœ… Switch between **beam search** (accurate) and **sampling** (creative) generation  
âœ… Support for **batch captioning** in the notebook  
âœ… GPU acceleration when available  
âœ… Clean, user-friendly UI with copy-to-clipboard  
âœ… Detailed help tooltips in settings  
âœ… Image caption history in the Streamlit app

---

## ğŸ“¸ Screenshots

### Streamlit Web App
![Streamlit App Screenshot](image/image_captioning_app_1.jpg)
![Streamlit App Screenshot](image/image_captioning_app_2.jpg)
![Streamlit App Screenshot](image/image_captioning_app_3.jpg)

---

## ğŸ›  Installation

### 1ï¸âƒ£ Clone the Repository

```bash
git clone https://github.com/antoningr/Image_to_Text_Captioning
cd Image_to_Text_Captioning
```

### 2ï¸âƒ£ Create a Virtual Environment (optional)

```bash
python -m venv venv
source venv/bin/activate   # On Linux/Mac
venv\Scripts\activate      # On Windows
```

### 3ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

---

## â–¶ï¸ Usage

### Run the Streamlit App

```bash
streamlit run app.py
```

Then open your browser at: http://localhost:8501


### Run the Jupyter Notebook

```bash
jupyter notebook notebook/image_captioning_blip.ipynb
```

---

## ğŸ§  How It Works

1. Model: BLIP from Hugging Face Transformers
2. Processor: Preprocesses the input image for the model
3. Caption Generation: Uses beam search (precise) or sampling (creative) decoding
4. Output: A descriptive text string generated from the image

Pipeline:

```bash
Image â†’ Processor â†’ BLIP Model â†’ Decoder â†’ Caption
```

---

## ğŸ“Š Example Results

| Image                      | Generated Caption                                           |
| -------------------------- | ----------------------------------------------------------- |
| ğŸ¶ Dog                     | "a dog sitting next to a bicycle on a porch"                |
| ğŸ€ Basketball              | "a basketball hoop with a ball in it"                       |
| ğŸ‘· Construction Workers    | "two construction workers standing in front of a building"  |
